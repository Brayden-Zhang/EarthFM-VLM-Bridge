nohup: ignoring input
Setting env var 'TORCH_NCCL_AVOID_RECORD_STREAMS' to '1'
Setting env var 'TORCH_DIST_INIT_BARRIER' to '1'
Global rank 0 = local rank 0 = file system local rank 0
2025-03-06 10:05:51.648	neptune-cs-aus-256.reviz.ai2.in:0	olmo_core.utils:200	INFO	Setting env var 'OMP_NUM_THREADS' to '8'
2025-03-06 10:05:51.648	neptune-cs-aus-256.reviz.ai2.in:0	olmo_core.utils:200	INFO	Setting env var 'TOKENIZERS_PARALLELISM' to 'false'
2025-03-06 10:05:51.648	neptune-cs-aus-256.reviz.ai2.in:0	olmo_core.train:135	INFO	Using distributed backend cpu:gloo,cuda:nccl
2025-03-06 10:05:51.648	neptune-cs-aus-256.reviz.ai2.in:0	helios.internal.experiment:117	INFO	Common overrides: []
2025-03-06 10:05:51.667	neptune-cs-aus-256.reviz.ai2.in:0	helios.internal.experiment:119	INFO	Common: CommonComponents(run_name='test_adjusted_patch_disc_11', save_folder='./local_output/checkpoints/yawenzzzz/test_adjusted_patch_disc_11', supported_modality_names=['sentinel2_l2a', 'latlon', 'sentinel1', 'worldcover'], launch=BeakerLaunchConfig(name='test_adjusted_patch_disc_11-train-733a401d', cmd=['scripts/latent_mim.py', 'train', 'test_adjusted_patch_disc_11', 'local'], budget='ai2/d5', task_name='train', workspace='ai2/earth-systems', description=None, setup_steps=['conda install gh --channel conda-forge', 'gh auth status', 'gh repo clone $REPO_URL .', 'git checkout "$GIT_REF"', 'git submodule update --init --recursive', 'conda shell.bash activate base', "pip install -e '.[all]'", 'pip install --upgrade beaker-py', 'pip freeze'], beaker_image='henryh/olmo-core-tch260cu124', num_nodes=1, num_gpus=1, shared_memory='10GiB', clusters=['local'], shared_filesystem=True, priority=<Priority.high: 'high'>, preemptible=True, retries=None, env_vars=[BeakerEnvVar(name='NCCL_DEBUG', value='WARN')], env_secrets=[BeakerEnvSecret(name='BEAKER_TOKEN', secret='yawenzzzz_BEAKER_TOKEN'), BeakerEnvSecret(name='WANDB_API_KEY', secret='yawenzzzz_WANDB_API_KEY'), BeakerEnvSecret(name='GITHUB_TOKEN', secret='yawenzzzz_GITHUB_TOKEN')], nfs=False, weka_buckets=[BeakerWekaBucket(bucket='dfive-default', mount='/weka/dfive-default')], allow_dirty=False, host_networking=None))
2025-03-06 10:05:51.668	neptune-cs-aus-256.reviz.ai2.in:0	helios.internal.experiment:138	INFO	Overrides: []
HeliosExperimentConfig(run_name='test_adjusted_patch_disc_11', launch=BeakerLaunchConfig(name='test_adjusted_patch_disc_11-train-733a401d', cmd=['scripts/latent_mim.py', 'train', 'test_adjusted_patch_disc_11', 'local'], budget='ai2/d5', task_name='train', workspace='ai2/earth-systems', description=None, setup_steps=['conda install gh --channel conda-forge', 'gh auth status', 'gh repo clone $REPO_URL .', 'git checkout "$GIT_REF"', 'git submodule update --init --recursive', 'conda shell.bash activate base', "pip install -e '.[all]'", 'pip install --upgrade beaker-py', 'pip freeze'], beaker_image='henryh/olmo-core-tch260cu124', num_nodes=1, num_gpus=1, shared_memory='10GiB', clusters=['local'], shared_filesystem=True, priority=<Priority.high: 'high'>, preemptible=True, retries=None, env_vars=[BeakerEnvVar(name='NCCL_DEBUG', value='WARN')], env_secrets=[BeakerEnvSecret(name='BEAKER_TOKEN', secret='yawenzzzz_BEAKER_TOKEN'), BeakerEnvSecret(name='WANDB_API_KEY', secret='yawenzzzz_WANDB_API_KEY'), BeakerEnvSecret(name='GITHUB_TOKEN', secret='yawenzzzz_GITHUB_TOKEN')], nfs=False, weka_buckets=[BeakerWekaBucket(bucket='dfive-default', mount='/weka/dfive-default')], allow_dirty=False, host_networking=None), model=LatentMIMConfig(encoder_config=EncoderConfig(supported_modality_names=['sentinel2_l2a', 'latlon', 'sentinel1', 'worldcover'], embedding_size=256, max_patch_size=8, num_heads=8, mlp_ratio=4.0, depth=4, drop_path=0.1, max_sequence_length=12, use_channel_embs=True, random_channel_embs=False), decoder_config=PredictorConfig(supported_modality_names=['sentinel2_l2a', 'latlon', 'sentinel1', 'worldcover'], encoder_embedding_size=256, decoder_embedding_size=256, depth=4, mlp_ratio=4.0, num_heads=8, max_sequence_length=12, drop_path=0.0, learnable_channel_embeddings=True, random_channel_embeddings=False, output_embedding_size=None), transform_type='flip_and_rotate', token_budget=1500, h_w_to_sample_min=5, h_w_to_sample_max=13), dataset=HeliosDatasetConfig(tile_path='/weka/dfive-default/helios/dataset/20250223', supported_modality_names=['sentinel2_l2a', 'latlon', 'sentinel1', 'worldcover'], samples=None, dtype='float32', normalize=True), data_loader=HeliosDataLoaderConfig(work_dir='./local_output/checkpoints/yawenzzzz/test_adjusted_patch_disc_11', global_batch_size=8, seed=3622, shuffle=True, num_threads=0, num_workers=0, prefetch_factor=None, target_device_type='cpu', drop_last=True), train_module=LatentMIMTrainModuleConfig(optim_config=AdamWConfig(group_overrides=None, compile=False, fixed_fields=['initial_lr'], lr=0.002, betas=[0.9, 0.999], eps=1e-08, weight_decay=0.02, foreach=None, fused=None), rank_microbatch_size=8, compile_model=False, float8_config=None, dp_config=DataParallelConfig(name='ddp', param_dtype=None, reduce_dtype='float32', num_replicas=None), ac_config=None, compile_loss=False, autocast_precision=None, max_grad_norm=1.0, scheduler=CosWithWarmup(lr_field='lr', initial_lr_field='initial_lr', warmup_steps=200, alpha_f=0.1, t_max=None, warmup_min_lr=0.0), state_dict_save_opts=None, state_dict_load_opts=None, loss_config=LossConfig(loss_config={'type': 'patch_discrimination'}), masking_config=MaskingConfig(strategy_config={'type': 'random', 'encode_ratio': 0.1, 'decode_ratio': 0.75}), token_exit_cfg={'sentinel2_l2a': 0, 'latlon': 0, 'sentinel1': 0, 'worldcover': 0}, ema_decay=[0.996, 1.0]), trainer=TrainerConfig(save_folder='./local_output/checkpoints/yawenzzzz/test_adjusted_patch_disc_11', work_dir='./local_output/checkpoints/yawenzzzz/test_adjusted_patch_disc_11', load_path=None, load_strategy='if_available', checkpointer=CheckpointerConfig(work_dir='./local_output/checkpoints/yawenzzzz/test_adjusted_patch_disc_11', save_overwrite=None, pre_download=False, save_thread_count=None, load_thread_count=None, throttle_uploads=False), device=None, save_overwrite=False, max_duration=Duration(value=50, unit='epochs'), cancel_check_interval=1, hard_stop=None, metrics_collect_interval=1, callbacks={'wandb': HeliosWandBCallback(enabled=True, name='test_adjusted_patch_disc_11', project='helios-debug', entity='eai-ai2', group=None, tags=None, notes=None, config=None, cancel_tags=['cancel', 'canceled', 'cancelled'], cancel_check_interval=None), 'speed_monitor': HeliosSpeedMonitorCallback(num_flops_per_token=None, device_peak_flops=None, _total_steps=0, _total_tokens=0, _start_time=0.0, _first_step=True, _step_last_logged=0.0, _batch_load_start=0.0, _batch_load_time=0.0, _step_tokens=0, _step_seq_len=0, _parallel_degree=1, _bps_avg=None), 'gpu_memory_monitor': GPUMemoryMonitorCallback(device_id=None, _num_alloc_retries=0), 'config_saver': ConfigSaverCallback(config=None, fname='config.json'), 'downstream_evaluator': DownstreamEvaluatorCallbackConfig(tasks=[DownstreamTaskConfig(name='m-eurosat', batch_size=128, num_workers=8, pooling_type=<PoolingType.MEAN: 'mean'>, norm_stats_from_pretrained=True)], eval_interval=100, eval_duration=Duration(value=10, unit='epochs'), enabled=True)}, async_bookkeeping=None, no_checkpoints=False, no_evals=False), visualize_config=HeliosVisualizeConfig(output_dir='local_output/checkpoints/yawenzzzz/test_adjusted_patch_disc_11/visualizations', num_samples=None, global_step=None, normalize_strategy=<Strategy.PREDEFINED: 'predefined'>, std_multiplier=2.0), init_seed=12536)
2025-03-06 10:05:51.746	neptune-cs-aus-256.reviz.ai2.in:0	helios.nn.flexihelios:1304	INFO	Encoder kwargs: {'embedding_size': 256, 'max_patch_size': 8, 'num_heads': 8, 'mlp_ratio': 4.0, 'depth': 4, 'drop_path': 0.1, 'max_sequence_length': 12, 'use_channel_embs': True, 'random_channel_embs': False, 'supported_modalities': [ModalitySpec(name='sentinel2_l2a', tile_resolution_factor=16, band_sets=[BandSet(bands=['B02', 'B03', 'B04', 'B08'], resolution_factor=16), BandSet(bands=['B05', 'B06', 'B07', 'B8A', 'B11', 'B12'], resolution_factor=32), BandSet(bands=['B01', 'B09'], resolution_factor=64)], is_multitemporal=True, ignore_when_parsing=False), ModalitySpec(name='latlon', tile_resolution_factor=0, band_sets=[BandSet(bands=['lat', 'lon'], resolution_factor=0)], is_multitemporal=False, ignore_when_parsing=True), ModalitySpec(name='sentinel1', tile_resolution_factor=16, band_sets=[BandSet(bands=['vv', 'vh'], resolution_factor=16)], is_multitemporal=True, ignore_when_parsing=False), ModalitySpec(name='worldcover', tile_resolution_factor=16, band_sets=[BandSet(bands=['B1'], resolution_factor=16)], is_multitemporal=False, ignore_when_parsing=False)]}
2025-03-06 10:05:51.746	neptune-cs-aus-256.reviz.ai2.in:0	helios.nn.flexihelios:533	INFO	modalities being used by model: ['sentinel2_l2a', 'latlon', 'sentinel1', 'worldcover']
2025-03-06 10:05:54.509	neptune-cs-aus-256.reviz.ai2.in:0	helios.nn.flexihelios:1345	INFO	Predictor kwargs: {'encoder_embedding_size': 256, 'decoder_embedding_size': 256, 'depth': 4, 'mlp_ratio': 4.0, 'num_heads': 8, 'max_sequence_length': 12, 'drop_path': 0.0, 'learnable_channel_embeddings': True, 'random_channel_embeddings': False, 'supported_modalities': [ModalitySpec(name='sentinel2_l2a', tile_resolution_factor=16, band_sets=[BandSet(bands=['B02', 'B03', 'B04', 'B08'], resolution_factor=16), BandSet(bands=['B05', 'B06', 'B07', 'B8A', 'B11', 'B12'], resolution_factor=32), BandSet(bands=['B01', 'B09'], resolution_factor=64)], is_multitemporal=True, ignore_when_parsing=False), ModalitySpec(name='latlon', tile_resolution_factor=0, band_sets=[BandSet(bands=['lat', 'lon'], resolution_factor=0)], is_multitemporal=False, ignore_when_parsing=True), ModalitySpec(name='sentinel1', tile_resolution_factor=16, band_sets=[BandSet(bands=['vv', 'vh'], resolution_factor=16)], is_multitemporal=True, ignore_when_parsing=False), ModalitySpec(name='worldcover', tile_resolution_factor=16, band_sets=[BandSet(bands=['B1'], resolution_factor=16)], is_multitemporal=False, ignore_when_parsing=False)]}
2025-03-06 10:05:54.510	neptune-cs-aus-256.reviz.ai2.in:0	helios.nn.flexihelios:533	INFO	modalities being used by model: ['sentinel2_l2a', 'latlon', 'sentinel1', 'worldcover']
2025-03-06 10:05:55.101	neptune-cs-aus-256.reviz.ai2.in:0	helios.train.train_module.train_module:176	INFO	number of parameters: 10,112,384
2025-03-06 10:05:55.101	neptune-cs-aus-256.reviz.ai2.in:0	olmo_core.distributed.parallel:232	INFO	Built 1D device mesh with shape (dp=1,)
2025-03-06 10:05:55.102	neptune-cs-aus-256.reviz.ai2.in:0	helios.train.train_module.train_module:179	INFO	Data parallel world size = 1
2025-03-06 10:05:55.105	neptune-cs-aus-256.reviz.ai2.in:0	helios.train.train_module.train_module:232	INFO	Applied DDP to the model
2025-03-06 10:05:55.105	neptune-cs-aus-256.reviz.ai2.in:0	helios.train.train_module.train_module:237	INFO	Initializing model weights...
2025-03-06 10:05:55.105	neptune-cs-aus-256.reviz.ai2.in:0	helios.train.train_module.train_module:241	INFO	Building optimizer(s)...
2025-03-06 10:05:55.106	neptune-cs-aus-256.reviz.ai2.in:0	olmo_core.optim.config:172	INFO	Building AdamW optimizer with 1 param group(s)...
2025-03-06 10:05:55.107	neptune-cs-aus-256.reviz.ai2.in:0	olmo_core.optim.config:180	INFO	Group 0, 159 parameter(s):
 - lr: 0.002
 - betas: [0.9, 0.999]
 - eps: 1e-08
 - weight_decay: 0.02
 - amsgrad: False
 - foreach: None
 - maximize: False
 - capturable: False
 - differentiable: False
 - fused: None
 - initial_lr: 0.002
2025-03-06 10:05:55.107	neptune-cs-aus-256.reviz.ai2.in:0	helios.data.dataset:738	INFO	HeliosDataset kwargs: {'tile_path': PosixUPath('/weka/dfive-default/helios/dataset/20250223'), 'dtype': 'float32', 'normalize': True, 'supported_modalities': [ModalitySpec(name='sentinel2_l2a', tile_resolution_factor=16, band_sets=[BandSet(bands=['B02', 'B03', 'B04', 'B08'], resolution_factor=16), BandSet(bands=['B05', 'B06', 'B07', 'B8A', 'B11', 'B12'], resolution_factor=32), BandSet(bands=['B01', 'B09'], resolution_factor=64)], is_multitemporal=True, ignore_when_parsing=False), ModalitySpec(name='latlon', tile_resolution_factor=0, band_sets=[BandSet(bands=['lat', 'lon'], resolution_factor=0)], is_multitemporal=False, ignore_when_parsing=True), ModalitySpec(name='sentinel1', tile_resolution_factor=16, band_sets=[BandSet(bands=['vv', 'vh'], resolution_factor=16)], is_multitemporal=True, ignore_when_parsing=False), ModalitySpec(name='worldcover', tile_resolution_factor=16, band_sets=[BandSet(bands=['B1'], resolution_factor=16)], is_multitemporal=False, ignore_when_parsing=False)]}
2025-03-06 10:05:55.107	neptune-cs-aus-256.reviz.ai2.in:0	helios.dataset.parse:173	WARNING	ignoring modality naip not in supported_modalities
2025-03-06 10:05:55.107	neptune-cs-aus-256.reviz.ai2.in:0	helios.dataset.parse:199	INFO	Parsing sentinel1 year /weka/dfive-default/helios/dataset/20250223/10_sentinel1_monthly.csv
2025-03-06 10:06:04.130	neptune-cs-aus-256.reviz.ai2.in:0	helios.dataset.parse:199	INFO	Parsing sentinel1 two_week /weka/dfive-default/helios/dataset/20250223/10_sentinel1_freq.csv
2025-03-06 10:06:07.890	neptune-cs-aus-256.reviz.ai2.in:0	helios.dataset.parse:173	WARNING	ignoring modality sentinel2 not in supported_modalities
2025-03-06 10:06:07.890	neptune-cs-aus-256.reviz.ai2.in:0	helios.dataset.parse:199	INFO	Parsing sentinel2_l2a year /weka/dfive-default/helios/dataset/20250223/10_sentinel2_l2a_monthly.csv
2025-03-06 10:06:21.268	neptune-cs-aus-256.reviz.ai2.in:0	helios.dataset.parse:199	INFO	Parsing sentinel2_l2a two_week /weka/dfive-default/helios/dataset/20250223/10_sentinel2_l2a_freq.csv
2025-03-06 10:06:31.873	neptune-cs-aus-256.reviz.ai2.in:0	helios.dataset.parse:199	INFO	Parsing worldcover static /weka/dfive-default/helios/dataset/20250223/10_worldcover.csv
2025-03-06 10:06:42.116	neptune-cs-aus-256.reviz.ai2.in:0	helios.data.dataset:438	INFO	Total samples: 161746
2025-03-06 10:06:42.117	neptune-cs-aus-256.reviz.ai2.in:0	helios.data.dataset:439	INFO	Distribution of samples before filtering:

2025-03-06 10:06:42.382	neptune-cs-aus-256.reviz.ai2.in:0	helios.data.dataset:422	INFO	Modality sentinel1: 112431 samples (69.5%)
2025-03-06 10:06:42.382	neptune-cs-aus-256.reviz.ai2.in:0	helios.data.dataset:422	INFO	Modality sentinel2_l2a: 149622 samples (92.5%)
2025-03-06 10:06:42.382	neptune-cs-aus-256.reviz.ai2.in:0	helios.data.dataset:422	INFO	Modality worldcover: 137029 samples (84.7%)
2025-03-06 10:06:42.382	neptune-cs-aus-256.reviz.ai2.in:0	helios.data.dataset:427	INFO
Modality combinations:
2025-03-06 10:06:42.382	neptune-cs-aus-256.reviz.ai2.in:0	helios.data.dataset:430	INFO	sentinel1+sentinel2_l2a+worldcover: 96778 samples (59.8%)
2025-03-06 10:06:42.382	neptune-cs-aus-256.reviz.ai2.in:0	helios.data.dataset:430	INFO	sentinel1+sentinel2_l2a: 3529 samples (2.2%)
2025-03-06 10:06:42.382	neptune-cs-aus-256.reviz.ai2.in:0	helios.data.dataset:430	INFO	sentinel2_l2a+worldcover: 28584 samples (17.7%)
2025-03-06 10:06:42.382	neptune-cs-aus-256.reviz.ai2.in:0	helios.data.dataset:430	INFO	sentinel2_l2a: 20731 samples (12.8%)
2025-03-06 10:06:42.382	neptune-cs-aus-256.reviz.ai2.in:0	helios.data.dataset:430	INFO	sentinel1+worldcover: 11667 samples (7.2%)
2025-03-06 10:06:42.383	neptune-cs-aus-256.reviz.ai2.in:0	helios.data.dataset:430	INFO	sentinel1: 457 samples (0.3%)
2025-03-06 10:06:42.389	neptune-cs-aus-256.reviz.ai2.in:0	helios.data.dataset:447	INFO	Number of samples before filtering: 161746
2025-03-06 10:06:44.450	neptune-cs-aus-256.reviz.ai2.in:0	helios.data.dataset:490	INFO	Number of samples after filtering: 33104
2025-03-06 10:06:44.450	neptune-cs-aus-256.reviz.ai2.in:0	helios.data.dataset:491	INFO	Distribution of samples after filtering:
2025-03-06 10:06:44.525	neptune-cs-aus-256.reviz.ai2.in:0	helios.data.dataset:422	INFO	Modality sentinel1: 33104 samples (100.0%)
2025-03-06 10:06:44.525	neptune-cs-aus-256.reviz.ai2.in:0	helios.data.dataset:422	INFO	Modality sentinel2_l2a: 33104 samples (100.0%)
2025-03-06 10:06:44.525	neptune-cs-aus-256.reviz.ai2.in:0	helios.data.dataset:422	INFO	Modality worldcover: 33104 samples (100.0%)
2025-03-06 10:06:44.525	neptune-cs-aus-256.reviz.ai2.in:0	helios.data.dataset:427	INFO
Modality combinations:
2025-03-06 10:06:44.525	neptune-cs-aus-256.reviz.ai2.in:0	helios.data.dataset:430	INFO	sentinel1+sentinel2_l2a+worldcover: 33104 samples (100.0%)
2025-03-06 10:06:45.417	neptune-cs-aus-256.reviz.ai2.in:0	olmo_core.train.trainer:311	INFO	Creating new process group for async bookkeeping
2025-03-06 10:06:45.482	neptune-cs-aus-256.reviz.ai2.in:0	olmo_core.train.trainer:556	WARNING	No checkpoint found in save folder ('./local_output/checkpoints/yawenzzzz/test_adjusted_patch_disc_11') or load path ('None'), will train from scratch...
2025-03-06 10:06:45.482	neptune-cs-aus-256.reviz.ai2.in:0	olmo_core.train.trainer:563	INFO	Callback order:
2025-03-06 10:06:45.482	neptune-cs-aus-256.reviz.ai2.in:0	olmo_core.train.trainer:565	INFO	  - Callback 1: checkpointer
2025-03-06 10:06:45.482	neptune-cs-aus-256.reviz.ai2.in:0	olmo_core.train.trainer:565	INFO	  - Callback 2: downstream_evaluator
2025-03-06 10:06:45.482	neptune-cs-aus-256.reviz.ai2.in:0	olmo_core.train.trainer:565	INFO	  - Callback 3: wandb
2025-03-06 10:06:45.482	neptune-cs-aus-256.reviz.ai2.in:0	olmo_core.train.trainer:565	INFO	  - Callback 4: config_saver
2025-03-06 10:06:45.482	neptune-cs-aus-256.reviz.ai2.in:0	olmo_core.train.trainer:565	INFO	  - Callback 5: console_logger
2025-03-06 10:06:45.482	neptune-cs-aus-256.reviz.ai2.in:0	olmo_core.train.trainer:565	INFO	  - Callback 6: garbage_collector
2025-03-06 10:06:45.483	neptune-cs-aus-256.reviz.ai2.in:0	olmo_core.train.trainer:565	INFO	  - Callback 7: gpu_memory_monitor
2025-03-06 10:06:45.483	neptune-cs-aus-256.reviz.ai2.in:0	olmo_core.train.trainer:565	INFO	  - Callback 8: speed_monitor
2025-03-06 10:06:45.483	neptune-cs-aus-256.reviz.ai2.in:0	olmo_core.train.trainer:567	INFO	Training for 206,900 steps
2025-03-06 10:06:45.483	neptune-cs-aus-256.reviz.ai2.in:0	olmo_core.train.callbacks.checkpointer:182	INFO	Creating new process group for checkpointing (needed for async checkpointing)
2025-03-06 10:06:45.504	neptune-cs-aus-256.reviz.ai2.in:0	olmo_core.train.trainer:750	INFO	Saving checkpoint for step 0 to 'local_output/checkpoints/yawenzzzz/test_adjusted_patch_disc_11/step0' asynchronously...
2025-03-06 10:06:46.194	neptune-cs-aus-256.reviz.ai2.in:0	olmo_core.train.trainer:759	INFO	Checkpoint for step 0 saved successfully
wandb: Currently logged in as: yawenz (eai-ai2) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in local_output/checkpoints/yawenzzzz/test_adjusted_patch_disc_11/wandb/wandb/run-20250306_100647-qyhqeqgv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run test_adjusted_patch_disc_11
wandb: ‚≠êÔ∏è View project at https://wandb.ai/eai-ai2/helios-debug
wandb: üöÄ View run at https://wandb.ai/eai-ai2/helios-debug/runs/qyhqeqgv
2025-03-06 10:06:48.199	neptune-cs-aus-256.reviz.ai2.in:0	helios.train.callbacks.wandb:59	INFO	Gathering locations of entire dataset
2025-03-06 10:06:57.063	neptune-cs-aus-256.reviz.ai2.in:0	helios.train.callbacks.wandb:62	INFO	Uploading dataset distribution to wandb: (33104, 2)
2025-03-06 10:06:57.701	neptune-cs-aus-256.reviz.ai2.in:0	helios.train.callbacks.wandb:75	INFO	Gathering normalized data distribution

  0%|          | 0/100 [00:00<?, ?it/s]
  1%|          | 1/100 [00:00<00:48,  2.05it/s]
  2%|‚ñè         | 2/100 [00:00<00:44,  2.20it/s]
  3%|‚ñé         | 3/100 [00:01<00:43,  2.22it/s]
  4%|‚ñç         | 4/100 [00:01<00:42,  2.27it/s]
  5%|‚ñå         | 5/100 [00:02<00:42,  2.26it/s]
  6%|‚ñå         | 6/100 [00:02<00:41,  2.28it/s]
  7%|‚ñã         | 7/100 [00:03<00:41,  2.27it/s]
  8%|‚ñä         | 8/100 [00:03<00:41,  2.24it/s]
  9%|‚ñâ         | 9/100 [00:04<00:40,  2.23it/s]
 10%|‚ñà         | 10/100 [00:04<00:39,  2.25it/s]
 11%|‚ñà         | 11/100 [00:04<00:39,  2.27it/s]
 12%|‚ñà‚ñè        | 12/100 [00:05<00:38,  2.27it/s]
 13%|‚ñà‚ñé        | 13/100 [00:05<00:38,  2.23it/s]
 14%|‚ñà‚ñç        | 14/100 [00:06<00:38,  2.22it/s]
 15%|‚ñà‚ñå        | 15/100 [00:06<00:37,  2.24it/s]
 16%|‚ñà‚ñå        | 16/100 [00:07<00:37,  2.23it/s]
 17%|‚ñà‚ñã        | 17/100 [00:07<00:36,  2.26it/s]
 18%|‚ñà‚ñä        | 18/100 [00:08<00:36,  2.25it/s]
 19%|‚ñà‚ñâ        | 19/100 [00:08<00:35,  2.28it/s]
 20%|‚ñà‚ñà        | 20/100 [00:08<00:37,  2.15it/s]
 21%|‚ñà‚ñà        | 21/100 [00:09<00:36,  2.18it/s]
 22%|‚ñà‚ñà‚ñè       | 22/100 [00:09<00:35,  2.20it/s]
 23%|‚ñà‚ñà‚ñé       | 23/100 [00:10<00:35,  2.18it/s]
 24%|‚ñà‚ñà‚ñç       | 24/100 [00:10<00:34,  2.22it/s]
 25%|‚ñà‚ñà‚ñå       | 25/100 [00:11<00:34,  2.20it/s]
 26%|‚ñà‚ñà‚ñå       | 26/100 [00:11<00:33,  2.20it/s]
 27%|‚ñà‚ñà‚ñã       | 27/100 [00:12<00:33,  2.21it/s]
 28%|‚ñà‚ñà‚ñä       | 28/100 [00:12<00:32,  2.23it/s]
 29%|‚ñà‚ñà‚ñâ       | 29/100 [00:13<00:31,  2.23it/s]
 30%|‚ñà‚ñà‚ñà       | 30/100 [00:13<00:31,  2.24it/s]
 31%|‚ñà‚ñà‚ñà       | 31/100 [00:13<00:30,  2.27it/s]
 32%|‚ñà‚ñà‚ñà‚ñè      | 32/100 [00:14<00:30,  2.26it/s]
 33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:14<00:29,  2.27it/s]
 34%|‚ñà‚ñà‚ñà‚ñç      | 34/100 [00:15<00:29,  2.26it/s]
 35%|‚ñà‚ñà‚ñà‚ñå      | 35/100 [00:15<00:28,  2.25it/s]
 36%|‚ñà‚ñà‚ñà‚ñå      | 36/100 [00:16<00:28,  2.28it/s]
 37%|‚ñà‚ñà‚ñà‚ñã      | 37/100 [00:16<00:27,  2.28it/s]
 38%|‚ñà‚ñà‚ñà‚ñä      | 38/100 [00:16<00:27,  2.29it/s]
 39%|‚ñà‚ñà‚ñà‚ñâ      | 39/100 [00:17<00:26,  2.29it/s]
 40%|‚ñà‚ñà‚ñà‚ñà      | 40/100 [00:17<00:26,  2.28it/s]
 41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:18<00:26,  2.21it/s]
 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/100 [00:18<00:26,  2.16it/s]
 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 43/100 [00:19<00:26,  2.18it/s]
 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 44/100 [00:19<00:25,  2.20it/s]
 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 45/100 [00:20<00:25,  2.20it/s]
 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 46/100 [00:20<00:24,  2.18it/s]
 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 47/100 [00:21<00:24,  2.17it/s]
 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/100 [00:21<00:23,  2.17it/s]
 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:22<00:23,  2.17it/s]
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 50/100 [00:22<00:22,  2.20it/s]
 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/100 [00:22<00:22,  2.22it/s]
 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 52/100 [00:23<00:21,  2.22it/s]
 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 53/100 [00:23<00:21,  2.18it/s]
 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 54/100 [00:24<00:21,  2.19it/s]
 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 55/100 [00:24<00:20,  2.15it/s]
 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 56/100 [00:25<00:20,  2.17it/s]
 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:25<00:19,  2.19it/s]
 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 58/100 [00:26<00:18,  2.22it/s]
 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 59/100 [00:26<00:18,  2.25it/s]
 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 60/100 [00:26<00:17,  2.24it/s]
 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 61/100 [00:27<00:17,  2.24it/s]
 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 62/100 [00:27<00:17,  2.23it/s]
 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 63/100 [00:28<00:16,  2.26it/s]
 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 64/100 [00:28<00:16,  2.24it/s]
 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:29<00:15,  2.23it/s]
 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/100 [00:29<00:15,  2.22it/s]
 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 67/100 [00:30<00:15,  2.20it/s]
 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 68/100 [00:30<00:14,  2.20it/s]
 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 69/100 [00:31<00:13,  2.22it/s]
 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 70/100 [00:31<00:13,  2.26it/s]
 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 71/100 [00:31<00:12,  2.24it/s]
 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/100 [00:32<00:12,  2.23it/s]
 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:32<00:12,  2.23it/s]
 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 74/100 [00:33<00:11,  2.24it/s]
 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 75/100 [00:33<00:10,  2.28it/s]
 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 76/100 [00:34<00:10,  2.26it/s]
 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 77/100 [00:34<00:10,  2.15it/s]
 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 78/100 [00:35<00:10,  2.13it/s]
 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 79/100 [00:35<00:09,  2.12it/s]
 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 80/100 [00:36<00:09,  2.15it/s]
 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:36<00:08,  2.13it/s]
 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 82/100 [00:36<00:08,  2.15it/s]
 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 83/100 [00:37<00:07,  2.17it/s]
 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 84/100 [00:37<00:07,  2.21it/s]
 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 85/100 [00:38<00:06,  2.24it/s]
 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 86/100 [00:38<00:06,  2.24it/s]
 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 87/100 [00:39<00:05,  2.23it/s]
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 88/100 [00:39<00:05,  2.22it/s]
 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:40<00:04,  2.21it/s]
 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 90/100 [00:40<00:04,  2.23it/s]
 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 91/100 [00:40<00:04,  2.24it/s]
 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 92/100 [00:41<00:03,  2.22it/s]
 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 93/100 [00:41<00:03,  2.20it/s]
 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 94/100 [00:42<00:02,  2.16it/s]
 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 95/100 [00:42<00:02,  2.14it/s]
 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/100 [00:43<00:01,  2.15it/s]
 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:43<00:01,  2.15it/s]
 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 98/100 [00:44<00:00,  2.16it/s]
 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 99/100 [00:44<00:00,  2.17it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:45<00:00,  2.17it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:45<00:00,  2.21it/s]
2025-03-06 10:07:46.618	neptune-cs-aus-256.reviz.ai2.in:0	olmo_core.train.callbacks.garbage_collector:32	INFO	Automatic GC disabled for training, will run GC every 1000 steps
2025-03-06 10:07:46.624	neptune-cs-aus-256.reviz.ai2.in:0	olmo_core.train.callbacks.gpu_memory_monitor:41	INFO	GPU capacity: NVIDIA L40S with 44.53GiB memory of which 0.09GiB is currently allocated and 0.09GiB is currently reserved.
2025-03-06 10:07:46.624	neptune-cs-aus-256.reviz.ai2.in:0	helios.train.callbacks.speed_monitor:34	WARNING	Speed monitor callback bases token input based on token budget, encoder ratio, and decoder ratio
2025-03-06 10:07:46.625	neptune-cs-aus-256.reviz.ai2.in:0	helios.data.dataloader:257	INFO	Getting mock batch NOT FROM DATASET
2025-03-06 10:07:46.728	neptune-cs-aus-256.reviz.ai2.in:0	olmo_core.train.trainer:1019	INFO	Starting forward/backward dry-run batch...
2025-03-06 10:07:46.728	neptune-cs-aus-256.reviz.ai2.in:0	helios.train.train_module.latent_mim:184	INFO	Using ema decay 0.996
2025-03-06 10:07:46.768	neptune-cs-aus-256.reviz.ai2.in:0	helios.train.train_module.latent_mim:217	INFO	Training microbatch 1 of 1 with batch size 1
2025-03-06 10:07:47.925	neptune-cs-aus-256.reviz.ai2.in:0	helios.train.train_module.latent_mim:274	INFO	target encoder running here
2025-03-06 10:07:48.041	neptune-cs-aus-256.reviz.ai2.in:0	helios.train.loss:108	INFO	logit_mask: torch.Size([1, 956, 956])
2025-03-06 10:07:48.041	neptune-cs-aus-256.reviz.ai2.in:0	helios.train.loss:109	INFO	scores: torch.Size([1, 956, 956])
2025-03-06 10:07:48.184	neptune-cs-aus-256.reviz.ai2.in:0	olmo_core.train.trainer:1021	INFO	Dry-run complete
2025-03-06 10:07:48.201	neptune-cs-aus-256.reviz.ai2.in:0	helios.data.dataloader:143	INFO	Global data order indices saved to:
'local_output/checkpoints/yawenzzzz/test_adjusted_patch_disc_11/dataset-1cb0d73319611cc5619c972e4a8b5eaf9ee45efce624593e4552cc144bfb2860/global_indices_epoch1_seed3622_size33104.npy'
2025-03-06 10:07:48.202	neptune-cs-aus-256.reviz.ai2.in:0	olmo_core.train.trainer:1026	INFO	Starting epoch 1...
2025-03-06 10:07:49.732	neptune-cs-aus-256.reviz.ai2.in:0	helios.train.train_module.latent_mim:184	INFO	Using ema decay 0.9960000193330111
2025-03-06 10:07:49.736	neptune-cs-aus-256.reviz.ai2.in:0	helios.train.train_module.latent_mim:217	INFO	Training microbatch 1 of 1 with batch size 8
2025-03-06 10:07:49.812	neptune-cs-aus-256.reviz.ai2.in:0	helios.train.train_module.latent_mim:274	INFO	target encoder running here
2025-03-06 10:07:49.848	neptune-cs-aus-256.reviz.ai2.in:0	helios.train.loss:108	INFO	logit_mask: torch.Size([1, 7815, 7815])
2025-03-06 10:07:49.849	neptune-cs-aus-256.reviz.ai2.in:0	helios.train.loss:109	INFO	scores: torch.Size([1, 7815, 7815])
2025-03-06 10:07:50.174	neptune-cs-aus-256.reviz.ai2.in:0	olmo_core.train.callbacks.console_logger:62	INFO	[step=0/206900,epoch=1]
    train/Patch Discrimination=1.410
2025-03-06 10:07:50.175	neptune-cs-aus-256.reviz.ai2.in:0	olmo_core.train.callbacks.console_logger:62	INFO	[step=1/206900,epoch=1]
    optim/LR (group 0)=1.00E-05
    optim/total grad norm=0.5704
    system/GPU active mem (%)=4.191
    system/GPU active mem (GiB)=1.866
    system/GPU reserved mem (%)=5.136
    system/GPU reserved mem (GiB)=2.287
    throughput/device/data loading (s)=1.529
    train/Patch Discrimination=1.416
2025-03-06 10:07:51.524	neptune-cs-aus-256.reviz.ai2.in:0	helios.train.train_module.latent_mim:184	INFO	Using ema decay 0.9960000386660223
2025-03-06 10:07:51.528	neptune-cs-aus-256.reviz.ai2.in:0	py.warnings:109	WARNING	/weka/dfive-default/yawenz/helios/helios/train/train_module/latent_mim.py:211: UserWarning: called a synchronizing CUDA operation (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:150.)
  total_batch_loss = torch.tensor(0.0, device=self.device)

2025-03-06 10:07:51.529	neptune-cs-aus-256.reviz.ai2.in:0	helios.train.train_module.latent_mim:217	INFO	Training microbatch 1 of 1 with batch size 8
2025-03-06 10:07:51.546	neptune-cs-aus-256.reviz.ai2.in:0	py.warnings:109	WARNING	/weka/dfive-default/yawenz/helios/helios/data/dataset.py:150: UserWarning: called a synchronizing CUDA operation (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:150.)
  return tensor.to(device)

2025-03-06 10:07:51.557	neptune-cs-aus-256.reviz.ai2.in:0	py.warnings:109	WARNING	/weka/dfive-default/yawenz/helios/helios/train/masking.py:224: UserWarning: called a synchronizing CUDA operation (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:150.)
  mask = torch.as_tensor(flat_mask_tokens, device=device)

2025-03-06 10:07:51.560	neptune-cs-aus-256.reviz.ai2.in:0	py.warnings:109	WARNING	/weka/dfive-default/yawenz/helios/helios/nn/flexihelios.py:262: UserWarning: called a synchronizing CUDA operation (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:150.)
  if self.is_any_data_seen_by_encoder(token_mask):

2025-03-06 10:07:51.561	neptune-cs-aus-256.reviz.ai2.in:0	py.warnings:109	WARNING	/weka/dfive-default/yawenz/helios/helios/nn/flexihelios.py:263: UserWarning: called a synchronizing CUDA operation (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:150.)
  patchified_data = modality_data[..., channel_set_indices]

2025-03-06 10:07:51.561	neptune-cs-aus-256.reviz.ai2.in:0	py.warnings:109	WARNING	/weka/dfive-default/yawenz/helios/helios/nn/flexi_patch_embed.py:154: UserWarning: called a synchronizing CUDA operation (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:150.)
  pinv = pinv.to(patch_embed.device)

2025-03-06 10:07:51.573	neptune-cs-aus-256.reviz.ai2.in:0	py.warnings:109	WARNING	/weka/dfive-default/yawenz/helios/helios/nn/flexihelios.py:784: UserWarning: called a synchronizing CUDA operation (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:150.)
  x = x[:, :max_length]

2025-03-06 10:07:51.573	neptune-cs-aus-256.reviz.ai2.in:0	py.warnings:109	WARNING	/weka/dfive-default/yawenz/helios/helios/nn/flexihelios.py:786: UserWarning: called a synchronizing CUDA operation (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:150.)
  updated_mask = 1 - sorted_mask[:, :max_length]

2025-03-06 10:07:51.578	neptune-cs-aus-256.reviz.ai2.in:0	py.warnings:109	WARNING	/weka/dfive-default/yawenz/helios/helios/nn/flexihelios.py:832: UserWarning: called a synchronizing CUDA operation (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:150.)
  out[~full_mask.bool()] = x[~mask.bool()]

2025-03-06 10:07:51.583	neptune-cs-aus-256.reviz.ai2.in:0	py.warnings:109	WARNING	/weka/dfive-default/yawenz/helios/helios/nn/flexihelios.py:1114: UserWarning: called a synchronizing CUDA operation (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:150.)
  x = tokens[:, :max_length_to_be_decoded]

2025-03-06 10:07:51.583	neptune-cs-aus-256.reviz.ai2.in:0	py.warnings:109	WARNING	/weka/dfive-default/yawenz/helios/helios/nn/flexihelios.py:1115: UserWarning: called a synchronizing CUDA operation (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:150.)
  y = tokens[:, -max_length_of_unmasked_tokens:]

2025-03-06 10:07:51.583	neptune-cs-aus-256.reviz.ai2.in:0	py.warnings:109	WARNING	/weka/dfive-default/yawenz/helios/helios/nn/flexihelios.py:1121: UserWarning: called a synchronizing CUDA operation (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:150.)
  x_mask = binarized_decoder_mask[:, :max_length_to_be_decoded].to(

2025-03-06 10:07:51.584	neptune-cs-aus-256.reviz.ai2.in:0	py.warnings:109	WARNING	/weka/dfive-default/yawenz/helios/helios/nn/flexihelios.py:1126: UserWarning: called a synchronizing CUDA operation (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:150.)
  y_mask = binarized_online_encoder_mask[:, -max_length_of_unmasked_tokens:].to(

2025-03-06 10:07:51.588	neptune-cs-aus-256.reviz.ai2.in:0	py.warnings:109	WARNING	/weka/dfive-default/yawenz/helios/helios/nn/flexihelios.py:1247: UserWarning: called a synchronizing CUDA operation (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:150.)
  if self.is_any_data_to_be_decoded(modality_mask):

2025-03-06 10:07:51.589	neptune-cs-aus-256.reviz.ai2.in:0	helios.train.train_module.latent_mim:274	INFO	target encoder running here
2025-03-06 10:07:51.618	neptune-cs-aus-256.reviz.ai2.in:0	py.warnings:109	WARNING	/weka/dfive-default/yawenz/helios/helios/train/loss.py:86: UserWarning: called a synchronizing CUDA operation (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:150.)
  pred = all_preds[all_masks == MaskValue.DECODER.value].unsqueeze(dim=0)

2025-03-06 10:07:51.618	neptune-cs-aus-256.reviz.ai2.in:0	py.warnings:109	WARNING	/weka/dfive-default/yawenz/helios/helios/train/loss.py:87: UserWarning: called a synchronizing CUDA operation (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:150.)
  target = all_targets[all_masks == MaskValue.DECODER.value].unsqueeze(dim=0)

2025-03-06 10:07:51.620	neptune-cs-aus-256.reviz.ai2.in:0	py.warnings:109	WARNING	/weka/dfive-default/yawenz/helios/helios/train/loss.py:106: UserWarning: called a synchronizing CUDA operation (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:150.)
  logit_mask[:, start:end, start:end] = 0

2025-03-06 10:07:51.621	neptune-cs-aus-256.reviz.ai2.in:0	helios.train.loss:108	INFO	logit_mask: torch.Size([1, 7383, 7383])
2025-03-06 10:07:51.621	neptune-cs-aus-256.reviz.ai2.in:0	helios.train.loss:109	INFO	scores: torch.Size([1, 7383, 7383])
2025-03-06 10:07:51.623	neptune-cs-aus-256.reviz.ai2.in:0	py.warnings:109	WARNING	/weka/dfive-default/yawenz/helios/helios/train/loss.py:39: UserWarning: called a synchronizing CUDA operation (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:150.)
  return torch.repeat_interleave(reciprocals, t)

2025-03-06 10:07:51.649	neptune-cs-aus-256.reviz.ai2.in:0	py.warnings:109	WARNING	/weka/dfive-default/yawenz/.venv-helios/lib/python3.10/site-packages/torch/autograd/graph.py:825: UserWarning: called a synchronizing CUDA operation (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:150.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

2025-03-06 10:07:52.088	neptune-cs-aus-256.reviz.ai2.in:0	olmo_core.train.callbacks.console_logger:62	INFO	[step=2/206900,epoch=1]
    optim/LR (group 0)=2.00E-05
    optim/total grad norm=0.7875
    system/GPU active mem (%)=3.884
    system/GPU active mem (GiB)=1.729
    system/GPU reserved mem (%)=5.141
    system/GPU reserved mem (GiB)=2.289
    throughput/device/BPS=0.5726
    throughput/device/BPS (estimated avg)=0.5726
    throughput/device/TPS Decoded=5,153
    throughput/device/TPS Decoded (estimated avg)=5,153
    throughput/device/TPS Encoded=687.2
    throughput/device/TPS Encoded (estimated avg)=687.2
    throughput/device/TPS Target Encoder=6,871
    throughput/device/TPS Target Encoder (estimated avg)=6,871
    throughput/device/data loading (%)=90.40
    throughput/device/data loading (s)=1.579
    throughput/total tokens decoded-since-restart=9,000
    throughput/total tokens encoded-since-restart=1,200
    throughput/total tokens target encoder-since-restart=12,000
    train/Patch Discrimination=1.406
2025-03-06 10:07:53.206	neptune-cs-aus-256.reviz.ai2.in:0	helios.train.train_module.latent_mim:184	INFO	Using ema decay 0.9960000579990334
2025-03-06 10:07:53.210	neptune-cs-aus-256.reviz.ai2.in:0	py.warnings:109	WARNING	/weka/dfive-default/yawenz/helios/helios/train/train_module/latent_mim.py:211: UserWarning: called a synchronizing CUDA operation (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:150.)
  total_batch_loss = torch.tensor(0.0, device=self.device)

2025-03-06 10:07:53.210	neptune-cs-aus-256.reviz.ai2.in:0	helios.train.train_module.latent_mim:217	INFO	Training microbatch 1 of 1 with batch size 8
2025-03-06 10:07:53.228	neptune-cs-aus-256.reviz.ai2.in:0	py.warnings:109	WARNING	/weka/dfive-default/yawenz/helios/helios/data/dataset.py:150: UserWarning: called a synchronizing CUDA operation (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:150.)
  return tensor.to(device)

2025-03-06 10:07:53.268	neptune-cs-aus-256.reviz.ai2.in:0	py.warnings:109	WARNING	/weka/dfive-default/yawenz/helios/helios/train/masking.py:224: UserWarning: called a synchronizing CUDA operation (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:150.)
  mask = torch.as_tensor(flat_mask_tokens, device=device)

2025-03-06 10:07:53.276	neptune-cs-aus-256.reviz.ai2.in:0	py.warnings:109	WARNING	/weka/dfive-default/yawenz/helios/helios/nn/flexihelios.py:262: UserWarning: called a synchronizing CUDA operation (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:150.)
  if self.is_any_data_seen_by_encoder(token_mask):

2025-03-06 10:07:53.276	neptune-cs-aus-256.reviz.ai2.in:0	py.warnings:109	WARNING	/weka/dfive-default/yawenz/helios/helios/nn/flexihelios.py:263: UserWarning: called a synchronizing CUDA operation (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:150.)
  patchified_data = modality_data[..., channel_set_indices]

2025-03-06 10:07:53.276	neptune-cs-aus-256.reviz.ai2.in:0	py.warnings:109	WARNING	/weka/dfive-default/yawenz/helios/helios/nn/flexi_patch_embed.py:154: UserWarning: called a synchronizing CUDA operation (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:150.)
  pinv = pinv.to(patch_embed.device)

2025-03-06 10:07:53.293	neptune-cs-aus-256.reviz.ai2.in:0	py.warnings:109	WARNING	/weka/dfive-default/yawenz/helios/helios/nn/flexihelios.py:784: UserWarning: called a synchronizing CUDA operation (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:150.)
  x = x[:, :max_length]

2025-03-06 10:07:53.293	neptune-cs-aus-256.reviz.ai2.in:0	py.warnings:109	WARNING	/weka/dfive-default/yawenz/helios/helios/nn/flexihelios.py:786: UserWarning: called a synchronizing CUDA operation (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:150.)
  updated_mask = 1 - sorted_mask[:, :max_length]

2025-03-06 10:07:53.298	neptune-cs-aus-256.reviz.ai2.in:0	py.warnings:109	WARNING	/weka/dfive-default/yawenz/helios/helios/nn/flexihelios.py:832: UserWarning: called a synchronizing CUDA operation (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:150.)
  out[~full_mask.bool()] = x[~mask.bool()]

2025-03-06 10:07:53.303	neptune-cs-aus-256.reviz.ai2.in:0	py.warnings:109	WARNING	/weka/dfive-default/yawenz/helios/helios/nn/flexihelios.py:1114: UserWarning: called a synchronizing CUDA operation (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:150.)
  x = tokens[:, :max_length_to_be_decoded]

2025-03-06 10:07:53.303	neptune-cs-aus-256.reviz.ai2.in:0	py.warnings:109	WARNING	/weka/dfive-default/yawenz/helios/helios/nn/flexihelios.py:1115: UserWarning: called a synchronizing CUDA operation (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:150.)
  y = tokens[:, -max_length_of_unmasked_tokens:]

2025-03-06 10:07:53.303	neptune-cs-aus-256.reviz.ai2.in:0	py.warnings:109	WARNING	/weka/dfive-default/yawenz/helios/helios/nn/flexihelios.py:1121: UserWarning: called a synchronizing CUDA operation (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:150.)
  x_mask = binarized_decoder_mask[:, :max_length_to_be_decoded].to(

2025-03-06 10:07:53.303	neptune-cs-aus-256.reviz.ai2.in:0	py.warnings:109	WARNING	/weka/dfive-default/yawenz/helios/helios/nn/flexihelios.py:1126: UserWarning: called a synchronizing CUDA operation (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:150.)
  y_mask = binarized_online_encoder_mask[:, -max_length_of_unmasked_tokens:].to(

2025-03-06 10:07:53.308	neptune-cs-aus-256.reviz.ai2.in:0	py.warnings:109	WARNING	/weka/dfive-default/yawenz/helios/helios/nn/flexihelios.py:1247: UserWarning: called a synchronizing CUDA operation (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:150.)
  if self.is_any_data_to_be_decoded(modality_mask):

2025-03-06 10:07:53.309	neptune-cs-aus-256.reviz.ai2.in:0	helios.train.train_module.latent_mim:274	INFO	target encoder running here
2025-03-06 10:07:53.345	neptune-cs-aus-256.reviz.ai2.in:0	py.warnings:109	WARNING	/weka/dfive-default/yawenz/helios/helios/train/loss.py:86: UserWarning: called a synchronizing CUDA operation (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:150.)
  pred = all_preds[all_masks == MaskValue.DECODER.value].unsqueeze(dim=0)

2025-03-06 10:07:53.346	neptune-cs-aus-256.reviz.ai2.in:0	py.warnings:109	WARNING	/weka/dfive-default/yawenz/helios/helios/train/loss.py:87: UserWarning: called a synchronizing CUDA operation (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:150.)
  target = all_targets[all_masks == MaskValue.DECODER.value].unsqueeze(dim=0)

2025-03-06 10:07:53.349	neptune-cs-aus-256.reviz.ai2.in:0	py.warnings:109	WARNING	/weka/dfive-default/yawenz/helios/helios/train/loss.py:106: UserWarning: called a synchronizing CUDA operation (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:150.)
  logit_mask[:, start:end, start:end] = 0

2025-03-06 10:07:53.350	neptune-cs-aus-256.reviz.ai2.in:0	helios.train.loss:108	INFO	logit_mask: torch.Size([1, 8869, 8869])
2025-03-06 10:07:53.350	neptune-cs-aus-256.reviz.ai2.in:0	helios.train.loss:109	INFO	scores: torch.Size([1, 8869, 8869])
2025-03-06 10:07:53.353	neptune-cs-aus-256.reviz.ai2.in:0	py.warnings:109	WARNING	/weka/dfive-default/yawenz/helios/helios/train/loss.py:39: UserWarning: called a synchronizing CUDA operation (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:150.)
  return torch.repeat_interleave(reciprocals, t)

2025-03-06 10:07:53.378	neptune-cs-aus-256.reviz.ai2.in:0	py.warnings:109	WARNING	/weka/dfive-default/yawenz/.venv-helios/lib/python3.10/site-packages/torch/autograd/graph.py:825: UserWarning: called a synchronizing CUDA operation (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:150.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

2025-03-06 10:07:53.847	neptune-cs-aus-256.reviz.ai2.in:0	olmo_core.train.callbacks.console_logger:62	INFO	[step=3/206900,epoch=1]
    optim/LR (group 0)=3.00E-05
    optim/total grad norm=0.5812
    system/GPU active mem (%)=5.210
    system/GPU active mem (GiB)=2.320
    system/GPU reserved mem (%)=7.483
    system/GPU reserved mem (GiB)=3.332
    throughput/device/BPS=0.5791
    throughput/device/BPS (estimated avg)=0.5759
    throughput/device/TPS Decoded=10,424
    throughput/device/TPS Decoded (estimated avg)=5,182
    throughput/device/TPS Encoded=1,389
    throughput/device/TPS Encoded (estimated avg)=691.1
    throughput/device/TPS Target Encoder=13,899
    throughput/device/TPS Target Encoder (estimated avg)=6,910
    throughput/device/data loading (%)=87.71
    throughput/device/data loading (s)=1.515
    throughput/total tokens decoded-since-restart=18,000
    throughput/total tokens encoded-since-restart=2,400
    throughput/total tokens target encoder-since-restart=24,000
    train/Patch Discrimination=1.443
2025-03-06 10:07:54.894	neptune-cs-aus-256.reviz.ai2.in:0	helios.train.train_module.latent_mim:184	INFO	Using ema decay 0.9960000773320444
2025-03-06 10:07:54.898	neptune-cs-aus-256.reviz.ai2.in:0	py.warnings:109	WARNING	/weka/dfive-default/yawenz/helios/helios/train/train_module/latent_mim.py:211: UserWarning: called a synchronizing CUDA operation (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:150.)
  total_batch_loss = torch.tensor(0.0, device=self.device)

2025-03-06 10:07:54.898	neptune-cs-aus-256.reviz.ai2.in:0	helios.train.train_module.latent_mim:217	INFO	Training microbatch 1 of 1 with batch size 8
